{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO04S24Rkzhy8UwYlwdZ7U4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ›  Exercises"],"metadata":{"id":"mgsTbBjWJ5S3"}},{"cell_type":"markdown","source":["## 1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n","- [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.\n","- [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs."],"metadata":{"id":"C8dhw7ddJ9GM"}},{"cell_type":"code","source":[],"metadata":{"id":"ZJr0iy9uKQEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Check out the [Keras guide on using pre-trained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n","- Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n","- It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen."],"metadata":{"id":"tw2wEYXIKEgd"}},{"cell_type":"code","source":[],"metadata":{"id":"1xNy3mz5KPhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Try replacing the TensorFlow Hub Universal Sentence Encoder pre-trained embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pre-trained on PubMed texts) pre-trained embedding. Does this affect results?\n","- Note: Using the BERT PubMed expert pre-trained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).\n","- Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf"],"metadata":{"id":"VFI2D7NyKKkD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElzU2g5FJ2PJ"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## 4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example, created a `X_of_Y` feature instead? Does this affect model performance?\n","- Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`."],"metadata":{"id":"PPgwbhlnKQuw"}},{"cell_type":"code","source":[],"metadata":{"id":"dLEG19KJKUyK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract, and return the abstract in the format:\n","- `PREDICTED_LABEL: SEQUENCE`\n","- `PREDICTED_LABEL: SEQUENCE`\n","- `PREDICTED_LABEL: SEQUENCE`\n","- `PREDICTED_LABEL: SEQUENCE`\n","- ...\n","    - You can find your own unstructured RCT abstract from PubMed or try this one from: [*Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection*](https://pubmed.ncbi.nlm.nih.gov/22244707/)."],"metadata":{"id":"3Y2vrtQPKVUp"}},{"cell_type":"code","source":[],"metadata":{"id":"zyQUbRgJKj33"},"execution_count":null,"outputs":[]}]}