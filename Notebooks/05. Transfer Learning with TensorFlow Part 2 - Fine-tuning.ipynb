{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJZmJ3oMCe8raaY1HZU3sX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 05. Transfer Learning with TensorFlow Part 2: Fine-tuning\n","\n","In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our models (even with less data).\n","\n","Now we're going to cover another type of transfer learning: fine-tuning.\n","\n","In **fine-tuning transfer learning** the pre-trained model weights from another model are unfrozen and tweaked to better suit your own data.\n","\n","For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n","\n","![image0](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-transfer-learning-feature-extraction-vs-fine-tuning.png)\n","*Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective.*"],"metadata":{"id":"oK0zuCMWD-H6"}},{"cell_type":"markdown","source":["## What we're going to cover\n","\n","We're going to go through the follow with TensorFlow:\n","\n","- Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n","- Using the Keras Functional API (a differnt way to build models in Keras)\n","- Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n","- Data augmentation (how to make your training dataset more diverse without adding more data)\n","- Running a series of modeling experiments on our Food Vision data\n","    - **Model 0**: a transfer learning model using the Keras Functional API\n","    - **Model 1**: a feature extraction transfer learning model on 1% of the data with data augmentation\n","    - **Model 2**: a feature extraction transfer learning model on 10% of the data with data augmentation\n","    - **Model 3**: a fine-tuned transfer learning model on 10% of the data\n","    - **Model 4**: a fine-tuned transfer learning model on 100% of the data\n","- Introduce the ModelCheckpoint callback to save intermediate training results\n","- Compare model experiment results using TensorBoard"],"metadata":{"id":"xNPodHMVEuHL"}},{"cell_type":"markdown","source":["## How you can use this notebook\n","\n","You can read through the descriptions and the code (it should all run, except for the cells that error on purpose), but there's a better option.\n","\n","Write all of the code yourself.\n","\n","Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, and why it breaks.\n","\n","You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n","\n","Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code."],"metadata":{"id":"S-mcxk5HFU7J"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFQO1C93D7-d","executionInfo":{"status":"ok","timestamp":1700623979433,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"22ad5249-cd79-4c03-8153-2931683131f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Notebook last run (end-to-end): 2023-11-22 03:32:59.136472\n"]}],"source":["import datetime\n","print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"]},{"cell_type":"markdown","source":["> ðŸ”‘ Note: As of TensorFlow 2.10+ there seem to be issues with the `tf.keras.applications.efficientnet` models (used later on) when loading weights via the `load_weights()` methods.\n",">\n","> To fix this, I've updated the code to use `tf.keras.applications.efficientnet_v2`, this is a small change but results in far fewer errors.\n",">\n","> You can see the [full write-up on the course GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/575)."],"metadata":{"id":"xsjgJQ3tFo0h"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","print(f\"TensorFlow version: {tf.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esMx70MQFnPY","executionInfo":{"status":"ok","timestamp":1700624071931,"user_tz":-540,"elapsed":6059,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"b8758a3c-faff-4e11-d363-7423d7be5d3d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.14.0\n"]}]},{"cell_type":"code","source":["# Are we using a GPU? (if not & you're using Google Colab, go to Runtime -> Change Runtime Type -> Harware Accelerator: GPU )\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLnTxJJ4F-HM","executionInfo":{"status":"ok","timestamp":1700624073368,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"8fb32712-2d91-47db-cd4c-ea014b9ee4e0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"markdown","source":["## Creating helper functions\n","\n","Throughout your machine learning experiments, you'll likely come across snippets of code you want to use over and over again.\n","\n","For example, a plotting function that plots a model's `history` object (see `plot_loss_curves()` below).\n","\n","You could recreate these functions over and over again.\n","\n","But as you might've guessed, rewriting the same functions becomes tedious.\n","\n","One of the solutions is to store them in a helper script such as [`helper_functions.py`](). And then import the necessary functionality when you need it.\n","\n","For example, you might write:\n","```\n","from helper_functions import plot_loss_curves\n","\n","...\n","\n","plot_loss_curves(history)\n","```\n","Let's see what this looks like."],"metadata":{"id":"4SeIhoeFGB_5"}},{"cell_type":"code","source":["# Get helper_functions.py script from course GitHub\n","!wget https://raw.githubusercontent.com/yhs2773/TensorFlow-for-Deep-Learning/main/helper_functions.py\n","\n","# Import helper functions we're going to use\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6alaEF11cc0","executionInfo":{"status":"ok","timestamp":1700636679419,"user_tz":-540,"elapsed":354,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"f281317c-fe5d-4ca2-c42c-5f148ab91283"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-22 07:04:39--  https://raw.githubusercontent.com/yhs2773/TensorFlow-for-Deep-Learning/main/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10872 (11K) [text/plain]\n","Saving to: â€˜helper_functions.pyâ€™\n","\n","\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.62K  --.-KB/s    in 0.001s  \n","\n","2023-11-22 07:04:39 (19.6 MB/s) - â€˜helper_functions.pyâ€™ saved [10872/10872]\n","\n"]}]},{"cell_type":"markdown","source":["Wonderful, now we've got a bunch of helper functions we can use throughout the notebook without having to rewrite them from scratch each time.\n","\n","> ðŸ”‘ Note: If you're running this notebook in Google Colab when it times out Colab will delete the `helper_functions.py` file. So to use the functions imported above, you'll have to rerun the cell."],"metadata":{"id":"TGlAvaze2IR3"}},{"cell_type":"markdown","source":["## 10 Food Classes: Working with less data\n","\n","We saw in the [previous notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb) that we could get great results with only 10% of the training data using transfer learning with TensorFlow Hub.\n","\n","In this notebook, we're going to continue to work with smaller subsets of the data, except this time we'll have a look at how we can use the in-built pre-trained models within the [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) module as well as how to fine-tune them to our custom dataset.\n","\n","We'll also practice using a new but similar dataloader function to what we've used before, [`image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) which is part of the [`tf.keras.utils`](https://www.tensorflow.org/api_docs/python/tf/keras/utils) module.\n","\n","Finally, we'll also be practicing using the [Keras Functional API](https://keras.io/guides/functional_api/) for building deep learning models. The Functional API is a more flexible way to create models than the `tf.keras.Sequential` API.\n","\n","We'll explore each of these in more detail as we go.\n","\n","Let's start by downloading some data."],"metadata":{"id":"HqFZ-KiT2PKU"}},{"cell_type":"code","source":["# Get 10% of the data of the 10 classes\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","\n","unzip_data(\"10_food_classes_10_percent.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zKhnzSf15o4","executionInfo":{"status":"ok","timestamp":1700636915730,"user_tz":-540,"elapsed":5440,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"ccc79659-f8b7-47f0-f765-2cd2e2693b42"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-22 07:08:30--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.217.207, 173.194.218.207, 108.177.12.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.217.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168546183 (161M) [application/zip]\n","Saving to: â€˜10_food_classes_10_percent.zipâ€™\n","\n","10_food_classes_10_ 100%[===================>] 160.74M  76.0MB/s    in 2.1s    \n","\n","2023-11-22 07:08:32 (76.0 MB/s) - â€˜10_food_classes_10_percent.zipâ€™ saved [168546183/168546183]\n","\n"]}]},{"cell_type":"markdown","source":["The dataset we're downloading is the 10 food classes dataset (from Food 101) with 10% of the training images we used in the previous notebook.\n","\n","> ðŸ”‘ Note: You can see how this dataset was created in the [image data modification notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb)."],"metadata":{"id":"w398E4Uc3DnZ"}},{"cell_type":"code","source":["# Walk through 10 percent data directory and list number of files\n","walk_through_dir(\"10_food_classes_10_percent\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-X50crqA295x","executionInfo":{"status":"ok","timestamp":1700636982030,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"fb750efc-a5bd-47a2-cb49-f8c558540533"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 2 directories and 0 images in '10_food_classes_10_percent'\n","There are 10 directories and 0 images in '10_food_classes_10_percent/test'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'\n","There are 10 directories and 0 images in '10_food_classes_10_percent/train'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'\n"]}]},{"cell_type":"markdown","source":["We can see that each of the training directories contains 75 images and each of the testing directories contains 250 images.\n","\n","Let's define our training and test file paths."],"metadata":{"id":"lRpGZ1JI3Rdp"}},{"cell_type":"code","source":["# Create training and test directories\n","train_dir = \"10_food_classes_10_percent/train/\"\n","test_dir = \"10_food_classes_10_percent/test/\""],"metadata":{"id":"0qjc10Wx3PaW","executionInfo":{"status":"ok","timestamp":1700637029236,"user_tz":-540,"elapsed":336,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Now we've got some image data, we need a way of loading it into a TensorFlow-compatible format.\n","\n","Previously, we've used the [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class.\n","\n","However, as of August 2023, this class is deprecated and isn't recommended for future usage (it's too slow).\n","\n","Because of this, we'll move on to using `tf.keras.utils.image_dataset_from_directory()`.\n","\n","This method expects image data in the following file format:\n","```\n","Example of file structure\n","\n","10_food_classes_10_percent <- top level folder\n","â””â”€â”€â”€train <- training images\n","â”‚   â””â”€â”€â”€pizza\n","â”‚   â”‚   â”‚   1008104.jpg\n","â”‚   â”‚   â”‚   1638227.jpg\n","â”‚   â”‚   â”‚   ...      \n","â”‚   â””â”€â”€â”€steak\n","â”‚       â”‚   1000205.jpg\n","â”‚       â”‚   1647351.jpg\n","â”‚       â”‚   ...\n","â”‚   \n","â””â”€â”€â”€test <- testing images\n","â”‚   â””â”€â”€â”€pizza\n","â”‚   â”‚   â”‚   1001116.jpg\n","â”‚   â”‚   â”‚   1507019.jpg\n","â”‚   â”‚   â”‚   ...      \n","â”‚   â””â”€â”€â”€steak\n","â”‚       â”‚   100274.jpg\n","â”‚       â”‚   1653815.jpg\n","â”‚       â”‚   ...    \n","```\n","One of the main benefits of using [`tf.keras.preprocessing.image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) rather than `ImageDataGenerator` is that it creates a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object rather than a generator.\n","\n","The main advantage of this is the `tf.data.Dataset` API is much more efficient (faster) than the `ImageDataGenerator` API which is paramount for larger datasets.\n","\n","Let's see it in action."],"metadata":{"id":"AIqyIOJN3chC"}},{"cell_type":"code","source":["# Create data inputs\n","import tensorflow as tf\n","\n","IMG_SIZE = (224, 224) # define image size\n","\n","train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n","                                                                            image_size=IMG_SIZE,\n","                                                                            label_mode=\"categorical\", # what type are the labels?\n","                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n","\n","test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n","                                                                           image_size=IMG_SIZE,\n","                                                                           label_mode=\"categorical\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_eMDqA-3a7O","executionInfo":{"status":"ok","timestamp":1700637359979,"user_tz":-540,"elapsed":868,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"8fe9ef7f-ccb6-468c-a7b6-a9a855b4b05d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 750 files belonging to 10 classes.\n","Found 2500 files belonging to 10 classes.\n"]}]},{"cell_type":"markdown","source":["Wonderful! Looks like our dataloaders have found the correct number of images for each dataset.\n","\n","For now, the main parameters we're concerned about in the `image_dataset_from_directory()` function are:\n","\n","- `directory` - the file path of the target directory we're loading images from.\n","- `image_size` - the target size of the images we're going to load in (height, width).\n","- `batch_size` - the batch size of the images we're going to load in. For example, if the `batch_size` is 32 (the default), batches of 32 images and labels at a time will be passed to the model.\n","\n","There are more we could play around with if we needed to in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory).\n","\n","If we check the training data datatype we should see it as a `BatchDataset` with shapes relating to our data."],"metadata":{"id":"LTBcpUa74yHi"}},{"cell_type":"code","source":["# Check the training data datatype\n","train_data_10_percent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W5-8ILf14rgI","executionInfo":{"status":"ok","timestamp":1700637479442,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"8b670b59-265f-4cd1-8693-339587005517"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["In the above output:\n","\n","- `(None, 224, 224, 3)` refers to the tensor shape of our images where `None` is the batch size, `224` is the height (and width), and `3` is the color channels (red, green, blue).\n","- `(None, 10)` refers to the tensor shape of the labels where `None` is the batch size and `10` is the number of possible labels (the 10 different food classes).\n","- Both image tensors and labels are of the datatype `tf.float32`.\n","\n","The `batch_size` is `None` due to it only being used during model training. You can think of `None` as a placeholder waiting to be filled with the `batch_size` parameter from `image_dataset_from_directory()`.\n","\n","Another benefit of using the tf.data.Dataset API is the associated method that comes with it.\n","\n","For example, if we want to find the names of the classes we were working with, we could use the `class_names` attribute."],"metadata":{"id":"X4X_vlrT5LJ7"}},{"cell_type":"code","source":["# Check out the class names of our dataset\n","train_data_10_percent.class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-L8J7yg5IyP","executionInfo":{"status":"ok","timestamp":1700637637417,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"5a05dc92-215a-4491-f395-c0a9357ec2d5"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['chicken_curry',\n"," 'chicken_wings',\n"," 'fried_rice',\n"," 'grilled_salmon',\n"," 'hamburger',\n"," 'ice_cream',\n"," 'pizza',\n"," 'ramen',\n"," 'steak',\n"," 'sushi']"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Or if we wanted to see an example batch of data, we could use the `take()` method."],"metadata":{"id":"-gSvRgZg5w_y"}},{"cell_type":"code","source":["# See an example batch of data\n","for images, labels in train_data_10_percent.take(1):\n","    print(images, labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJxhkRQE5vgR","executionInfo":{"status":"ok","timestamp":1700637659001,"user_tz":-540,"elapsed":1038,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"914dc690-36dd-4665-ac2b-a0524e408f04"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[2.60867348e+01 6.50000000e+00 8.14285755e+00]\n","   [1.78418350e+01 2.22448897e+00 4.58163166e+00]\n","   [1.58316336e+01 5.27040815e+00 4.33673429e+00]\n","   ...\n","   [6.38468895e+01 3.28468895e+01 3.84688902e+00]\n","   [5.70714302e+01 2.60714283e+01 0.00000000e+00]\n","   [6.03725395e+01 2.83725414e+01 3.37254119e+00]]\n","\n","  [[2.84693890e+01 9.01530647e+00 5.93367386e+00]\n","   [1.80306091e+01 1.74999785e+00 1.45918214e+00]\n","   [1.50918360e+01 3.48979568e+00 4.89285660e+00]\n","   ...\n","   [6.39999771e+01 3.29999771e+01 3.99997830e+00]\n","   [6.19949112e+01 3.09949093e+01 2.99491024e+00]\n","   [6.64745560e+01 3.44745560e+01 9.47455692e+00]]\n","\n","  [[5.67499962e+01 3.52499924e+01 1.95306091e+01]\n","   [2.73622437e+01 8.50510216e+00 3.31632710e+00]\n","   [1.57193890e+01 1.86734796e+00 4.82142973e+00]\n","   ...\n","   [5.94744873e+01 2.84744854e+01 2.60199547e-01]\n","   [5.86428566e+01 2.76428566e+01 4.28571701e-01]\n","   [5.99336815e+01 2.63622532e+01 1.36225319e+00]]\n","\n","  ...\n","\n","  [[1.64433624e+02 1.69433624e+02 1.15005035e+02]\n","   [1.62270370e+02 1.66285675e+02 1.15239754e+02]\n","   [1.60903076e+02 1.64071442e+02 1.16566322e+02]\n","   ...\n","   [1.23979523e+02 1.23979523e+02 7.18876877e+01]\n","   [1.27040764e+02 1.26826469e+02 7.72856750e+01]\n","   [1.25999939e+02 1.26428528e+02 7.65101395e+01]]\n","\n","  [[1.60928589e+02 1.65928589e+02 1.09928589e+02]\n","   [1.62653137e+02 1.67586823e+02 1.13785751e+02]\n","   [1.63647995e+02 1.67632690e+02 1.16948990e+02]\n","   ...\n","   [1.25357117e+02 1.25556076e+02 7.10714111e+01]\n","   [1.30158066e+02 1.29158066e+02 8.31581268e+01]\n","   [1.20525383e+02 1.21596794e+02 7.99030838e+01]]\n","\n","  [[1.63316345e+02 1.68316345e+02 1.10316345e+02]\n","   [1.57857178e+02 1.62857178e+02 1.07520454e+02]\n","   [1.62484695e+02 1.66484695e+02 1.15698982e+02]\n","   ...\n","   [1.19642792e+02 1.19428528e+02 6.47857361e+01]\n","   [1.28045807e+02 1.26974365e+02 8.26172791e+01]\n","   [1.18285645e+02 1.18285645e+02 8.22857056e+01]]]\n","\n","\n"," [[[8.57448959e+01 5.07448959e+01 3.07448978e+01]\n","   [8.72806091e+01 5.02806129e+01 3.22806129e+01]\n","   [8.82908096e+01 5.15051003e+01 3.39336739e+01]\n","   ...\n","   [1.93571533e+02 1.90000061e+02 2.06143005e+02]\n","   [1.95285767e+02 1.90285767e+02 2.10285767e+02]\n","   [2.03357147e+02 1.95357147e+02 2.16357147e+02]]\n","\n","  [[8.25204010e+01 4.95204086e+01 3.05204086e+01]\n","   [8.12193832e+01 4.62193871e+01 2.72193871e+01]\n","   [8.21275558e+01 4.69132652e+01 2.85561218e+01]\n","   ...\n","   [1.86944031e+02 1.83372559e+02 1.99714462e+02]\n","   [1.95357178e+02 1.87357178e+02 2.08357178e+02]\n","   [2.01734756e+02 1.93734756e+02 2.14734756e+02]]\n","\n","  [[7.55663223e+01 4.47142868e+01 2.81377544e+01]\n","   [7.72397995e+01 4.46122437e+01 2.79693890e+01]\n","   [7.61224518e+01 4.35510216e+01 2.63367348e+01]\n","   ...\n","   [1.73760330e+02 1.70188858e+02 1.86546066e+02]\n","   [1.86627594e+02 1.78627594e+02 1.99627594e+02]\n","   [1.99576752e+02 1.90576752e+02 2.11576752e+02]]\n","\n","  ...\n","\n","  [[1.87923416e+02 1.77923416e+02 1.52923416e+02]\n","   [1.87586670e+02 1.77586670e+02 1.52586670e+02]\n","   [1.89642776e+02 1.79642776e+02 1.54214203e+02]\n","   ...\n","   [8.50459061e+01 6.10459099e+01 3.49540901e+01]\n","   [8.73316269e+01 6.33316269e+01 3.93316269e+01]\n","   [8.29898224e+01 5.89898224e+01 3.49898224e+01]]\n","\n","  [[1.90357132e+02 1.80357132e+02 1.55357132e+02]\n","   [1.89846939e+02 1.79846939e+02 1.54846939e+02]\n","   [1.88857117e+02 1.78857117e+02 1.53428543e+02]\n","   ...\n","   [8.23011322e+01 5.87296638e+01 3.47296638e+01]\n","   [8.55917664e+01 6.15917664e+01 3.75917664e+01]\n","   [7.98521271e+01 5.58521271e+01 3.18521290e+01]]\n","\n","  [[1.88857254e+02 1.78857254e+02 1.53857254e+02]\n","   [1.91071564e+02 1.81071564e+02 1.56071564e+02]\n","   [1.86780716e+02 1.76780716e+02 1.51352142e+02]\n","   ...\n","   [8.40000000e+01 6.20000000e+01 3.90000000e+01]\n","   [8.08621674e+01 5.68621674e+01 3.28621674e+01]\n","   [8.21885605e+01 5.81885605e+01 3.41885605e+01]]]\n","\n","\n"," [[[1.99879471e+02 1.64879471e+02 1.22879463e+02]\n","   [1.97090073e+02 1.63351242e+02 1.18351242e+02]\n","   [1.96000000e+02 1.62317123e+02 1.17046875e+02]\n","   ...\n","   [1.42046387e+02 8.47879868e+01 5.17970505e+01]\n","   [1.40791626e+02 8.62693024e+01 5.05304642e+01]\n","   [1.42834854e+02 8.88348541e+01 5.28348579e+01]]\n","\n","  [[2.02140625e+02 1.68707596e+02 1.25924103e+02]\n","   [1.98709824e+02 1.65709824e+02 1.20709816e+02]\n","   [1.96783478e+02 1.63997757e+02 1.18569199e+02]\n","   ...\n","   [1.42522827e+02 8.31870804e+01 4.97834816e+01]\n","   [1.40839462e+02 8.48394547e+01 4.98394547e+01]\n","   [1.43707626e+02 8.81406631e+01 5.29241409e+01]]\n","\n","  [[2.01388397e+02 1.71000000e+02 1.27000000e+02]\n","   [1.98815201e+02 1.68732620e+02 1.22426811e+02]\n","   [1.96934479e+02 1.65785721e+02 1.19571426e+02]\n","   ...\n","   [1.43471603e+02 8.35059509e+01 4.89338493e+01]\n","   [1.43848221e+02 8.57656403e+01 5.14598351e+01]\n","   [1.44993515e+02 8.96051254e+01 5.42993240e+01]]\n","\n","  ...\n","\n","  [[1.79460449e+02 1.33531876e+02 6.31747284e+01]\n","   [1.89289841e+02 1.32575562e+02 6.55041351e+01]\n","   [1.77266235e+02 1.17868958e+02 5.26033134e+01]\n","   ...\n","   [1.90397308e+02 1.57248566e+02 1.16694801e+02]\n","   [1.91173553e+02 1.58173553e+02 1.15173546e+02]\n","   [1.89419708e+02 1.56419708e+02 1.12808136e+02]]\n","\n","  [[1.85563110e+02 1.35842911e+02 6.68429184e+01]\n","   [1.82135956e+02 1.26225899e+02 5.89534111e+01]\n","   [1.65462357e+02 1.09849815e+02 4.30154648e+01]\n","   ...\n","   [1.92477173e+02 1.58477173e+02 1.20691444e+02]\n","   [1.89216522e+02 1.56216522e+02 1.15216522e+02]\n","   [1.88433044e+02 1.55433044e+02 1.12433044e+02]]\n","\n","  [[1.75641006e+02 1.21946701e+02 5.44244194e+01]\n","   [1.69765640e+02 1.18982231e+02 5.02433701e+01]\n","   [1.61997253e+02 1.12809845e+02 4.42852745e+01]\n","   ...\n","   [1.91211945e+02 1.57211945e+02 1.19426208e+02]\n","   [1.87783417e+02 1.54783417e+02 1.13783417e+02]\n","   [1.88786179e+02 1.55786179e+02 1.12786186e+02]]]\n","\n","\n"," ...\n","\n","\n"," [[[2.10698990e+02 2.29714294e+02 2.20285721e+02]\n","   [2.18051025e+02 2.25877548e+02 2.20285721e+02]\n","   [2.25010193e+02 2.22709183e+02 2.22943878e+02]\n","   ...\n","   [1.13499084e+02 1.00927612e+02 8.37133484e+01]\n","   [8.76583633e+01 7.26583633e+01 5.36583633e+01]\n","   [1.28378098e+02 1.13378105e+02 9.43781052e+01]]\n","\n","  [[2.13214279e+02 2.29831619e+02 2.30280609e+02]\n","   [2.18066330e+02 2.26147949e+02 2.27852036e+02]\n","   [2.23469376e+02 2.26586731e+02 2.27714294e+02]\n","   ...\n","   [1.66841370e+02 1.55540283e+02 1.39213776e+02]\n","   [9.64334335e+01 8.30201263e+01 6.70201263e+01]\n","   [7.99951630e+01 6.39951630e+01 4.85920715e+01]]\n","\n","  [[1.84704086e+02 2.19418365e+02 2.26785721e+02]\n","   [2.02178574e+02 2.23811234e+02 2.29341843e+02]\n","   [2.17306122e+02 2.27642853e+02 2.29428574e+02]\n","   ...\n","   [1.95913010e+02 1.87127243e+02 1.72555847e+02]\n","   [1.51948578e+02 1.37948578e+02 1.25377151e+02]\n","   [8.64329529e+01 7.02186661e+01 5.78615265e+01]]\n","\n","  ...\n","\n","  [[8.58763885e+01 6.38763885e+01 5.05243149e+01]\n","   [7.11675873e+01 5.00961533e+01 3.33104401e+01]\n","   [9.41262436e+01 7.30803299e+01 5.52232285e+01]\n","   ...\n","   [2.86835480e+01 2.24692841e+01 1.14028778e+01]\n","   [1.82602463e+01 1.20459833e+01 2.66331625e+00]\n","   [2.28465290e+01 1.58465290e+01 7.86688423e+00]]\n","\n","  [[8.43116074e+01 5.80258408e+01 3.46686325e+01]\n","   [9.29693832e+01 6.58213882e+01 4.44642029e+01]\n","   [1.11423515e+02 8.52091904e+01 6.85663223e+01]\n","   ...\n","   [7.73828506e+01 6.72960434e+01 5.47399979e+01]\n","   [8.21840439e+01 7.18982773e+01 6.19697151e+01]\n","   [1.05391754e+02 9.50345459e+01 8.61774292e+01]]\n","\n","  [[9.37145615e+01 6.03573875e+01 2.63879051e+01]\n","   [8.66730957e+01 5.46270752e+01 2.63617630e+01]\n","   [9.02445984e+01 5.68210640e+01 3.85251274e+01]\n","   ...\n","   [4.59542503e+01 2.87348061e+01 1.81685448e+01]\n","   [5.35093765e+01 3.41521988e+01 2.71521969e+01]\n","   [3.79287796e+01 1.82144260e+01 1.25716028e+01]]]\n","\n","\n"," [[[4.87959213e+01 8.16683655e+01 1.36051025e+02]\n","   [3.46785736e+01 6.63265305e+01 1.18535713e+02]\n","   [4.23367348e+01 7.00612259e+01 1.18760208e+02]\n","   ...\n","   [6.52245178e+01 9.19388046e+01 9.16530914e+01]\n","   [6.21938210e+01 9.01938248e+01 9.32652664e+01]\n","   [5.99591255e+01 8.46019440e+01 9.26734772e+01]]\n","\n","  [[2.70612240e+01 6.23469391e+01 1.19132652e+02]\n","   [4.07142906e+01 7.39949036e+01 1.28280624e+02]\n","   [3.47500000e+01 6.44081650e+01 1.15693871e+02]\n","   ...\n","   [6.08418388e+01 9.01275558e+01 9.43724442e+01]\n","   [5.44439201e+01 8.35102310e+01 8.75867615e+01]\n","   [5.87854691e+01 8.35201263e+01 9.15916595e+01]]\n","\n","  [[4.58520393e+01 8.56377563e+01 1.45933670e+02]\n","   [4.44540787e+01 8.26122437e+01 1.39540817e+02]\n","   [4.59081650e+01 7.93316345e+01 1.33147964e+02]\n","   ...\n","   [5.43521156e+01 9.00459595e+01 1.01234703e+02]\n","   [5.75970497e+01 8.99133301e+01 9.64439316e+01]\n","   [6.37905426e+01 8.99996490e+01 9.72292633e+01]]\n","\n","  ...\n","\n","  [[1.51428747e+01 8.30104733e+00 1.25816565e+01]\n","   [1.63571606e+01 1.12857323e+01 1.52857323e+01]\n","   [1.87601891e+01 1.49081678e+01 1.80969353e+01]\n","   ...\n","   [2.12433640e+02 1.77076523e+02 1.24862221e+02]\n","   [2.15295944e+02 1.79938828e+02 1.27724525e+02]\n","   [2.13219360e+02 1.77862244e+02 1.25647949e+02]]\n","\n","  [[1.70459251e+01 1.16428747e+01 1.56428747e+01]\n","   [1.87908325e+01 1.37857323e+01 1.77857323e+01]\n","   [1.56989479e+01 1.36989479e+01 1.62703762e+01]\n","   ...\n","   [2.20545990e+02 1.87545990e+02 1.34545990e+02]\n","   [2.16653030e+02 1.83653030e+02 1.30653030e+02]\n","   [2.08831665e+02 1.75831665e+02 1.22831673e+02]]\n","\n","  [[1.50714331e+01 1.00714331e+01 1.40714331e+01]\n","   [1.42908220e+01 1.22142906e+01 1.52398014e+01]\n","   [1.19898033e+01 1.04183750e+01 1.27755175e+01]\n","   ...\n","   [2.16127518e+02 1.83127518e+02 1.30127518e+02]\n","   [2.15545898e+02 1.82545898e+02 1.29545898e+02]\n","   [2.06612213e+02 1.73612213e+02 1.20612213e+02]]]\n","\n","\n"," [[[2.63571415e+01 1.83571415e+01 5.35714293e+00]\n","   [2.90459175e+01 2.10459175e+01 8.04591846e+00]\n","   [2.96428566e+01 2.16428566e+01 8.64285755e+00]\n","   ...\n","   [2.22571442e+02 2.26357147e+02 2.08928558e+02]\n","   [2.23622452e+02 2.26622452e+02 2.07622452e+02]\n","   [2.19127563e+02 2.22127563e+02 2.03127563e+02]]\n","\n","  [[2.73571434e+01 1.93571434e+01 6.35714293e+00]\n","   [3.00051022e+01 2.20051022e+01 9.00510216e+00]\n","   [3.00153065e+01 2.20153065e+01 9.01530647e+00]\n","   ...\n","   [2.23142868e+02 2.26928574e+02 2.09499985e+02]\n","   [2.25785751e+02 2.28785751e+02 2.09785751e+02]\n","   [2.23117355e+02 2.26117355e+02 2.07117355e+02]]\n","\n","  [[2.77857151e+01 1.93571434e+01 8.57142830e+00]\n","   [2.94132652e+01 2.09846935e+01 1.01989794e+01]\n","   [2.85969391e+01 2.01683674e+01 9.38265324e+00]\n","   ...\n","   [2.26831635e+02 2.30617340e+02 2.13188751e+02]\n","   [2.26270401e+02 2.29270401e+02 2.10270401e+02]\n","   [2.20352066e+02 2.23352066e+02 2.04352066e+02]]\n","\n","  ...\n","\n","  [[3.58779564e+01 3.02298965e+01 1.48726864e+01]\n","   [2.52757225e+01 2.03470631e+01 6.48989868e+00]\n","   [1.41225462e+01 1.27653160e+01 1.68372124e-01]\n","   ...\n","   [1.09020416e+02 8.08061218e+01 4.07142944e+01]\n","   [1.04612160e+02 7.56121597e+01 3.51836319e+01]\n","   [1.06933640e+02 7.39234161e+01 3.50000000e+01]]\n","\n","  [[7.00715179e+01 5.35255547e+01 3.30969696e+01]\n","   [5.93062706e+01 4.62296982e+01 2.68878136e+01]\n","   [4.75716248e+01 3.64542580e+01 1.77858467e+01]\n","   ...\n","   [1.14647957e+02 8.63877335e+01 4.71887512e+01]\n","   [1.10637779e+02 7.88367081e+01 3.97704010e+01]\n","   [1.07071503e+02 7.30715027e+01 3.50715027e+01]]\n","\n","  [[7.96173782e+01 5.71581497e+01 3.38009720e+01]\n","   [7.57857132e+01 5.52652397e+01 3.16887398e+01]\n","   [7.25511322e+01 5.55612793e+01 3.29031296e+01]\n","   ...\n","   [1.10285583e+02 8.14284058e+01 4.26427002e+01]\n","   [1.13617195e+02 8.16171951e+01 4.36171951e+01]\n","   [1.09055939e+02 7.50559387e+01 3.80559387e+01]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n","[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]], shape=(32, 10), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["Notice how the image arrays come out as tensors of pixel values and the labels come out as one-hot encodings (e.g. `[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]` for `hamburger`)."],"metadata":{"id":"HyxjU7hp55gV"}},{"cell_type":"markdown","source":["### Model 0: Building a transfer learning model using the Keras Functional API"],"metadata":{"id":"QCZvU1e76FOl"}},{"cell_type":"code","source":[],"metadata":{"id":"95Qcma3K50gl"},"execution_count":null,"outputs":[]}]}